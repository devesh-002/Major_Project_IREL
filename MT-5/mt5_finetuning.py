# -*- coding: utf-8 -*-
"""MT5_finetuning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LGynQlsJFdQT4AN36TxBnbZbXh7bvND3
"""

# !pip install transformers
# !pip install sentencepiece
# !pip install datasets

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import MT5ForConditionalGeneration, T5Tokenizer
import torch
from datasets import load_dataset
# from apex.parallel import DistributedDataParallel as DDP

# model initializations from huggingface

#indicbart
tokenizer = AutoTokenizer.from_pretrained("ai4bharat/IndicBART", do_lower_case=False, use_fast=False, keep_accents=True)
model = AutoModelForSeq2SeqLM.from_pretrained("ai4bharat/IndicBART")

# mt5
# model = MT5ForConditionalGeneration.from_pretrained("google/mt5-base")
#model=torch.nn.DataParallel(model)
#tokenizer = T5Tokenizer.from_pretrained("google/mt5-base")

# Put the model on GPU if available.
if torch.cuda.is_available():
  model = model.to("cuda")

# from google.colab import drive
# drive.mount('/content/drive')

# !cp /content/drive/MyDrive/data.zip  .
# !unzip /content/data.zip

# HYPERPARAMETERS
article_length = 512
summary_length = 64
train_path = "data/eng_train.csv"
val_path = "data/eng_val.csv"
batch_size = 1
num_epochs = 3
learning_rate = 5e-5
number_beams_eval = 8
language = "english"

def process_data_to_model_inputs(batch):
  # tokenize the inputs and labels
  inputs = tokenizer(batch["Article"], padding="max_length", truncation=True, max_length=article_length)
  outputs = tokenizer(batch["Summary"], padding="max_length", truncation=True, max_length=summary_length)

  batch["input_ids"] = inputs.input_ids
  batch["attention_mask"] = inputs.attention_mask
  batch["decoder_input_ids"] = outputs.input_ids
  batch["decoder_attention_mask"] = outputs.attention_mask
  batch["labels"] = outputs.input_ids.copy()
  batch["labels"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch["labels"]]

  return batch

# from google.colab import drive
# drive.mount('/content/gdrive')

# !cp /content/gdrive/MyDrive/data.zip  .
# !unzip data.zip

from datasets import load_dataset
train_data = load_dataset("csv", data_files=train_path)
validation_data = load_dataset("csv", data_files=val_path)

train_data = train_data.map(
    process_data_to_model_inputs, 
    batched=True,
    remove_columns=["Article", "Summary"]
)

train_data.set_format(
    type="torch", columns=["input_ids", "attention_mask", "decoder_input_ids",
                           "decoder_attention_mask", "labels"],
)

train_data =  train_data["train"]

validation_data = validation_data.map(
    process_data_to_model_inputs,
    batched=True,
    remove_columns=["Article", "Summary"]
)

validation_data.set_format(
    type="torch", columns=["input_ids", "attention_mask", "decoder_input_ids",
                           "decoder_attention_mask", "labels"],
)

validation_data =  validation_data["train"]

next( iter( train_data ) )

from torch.utils.data import DataLoader
train_data      = DataLoader(train_data, batch_size=batch_size)
validation_data = DataLoader(validation_data, batch_size=batch_size)

from torch.nn import CrossEntropyLoss

loss_fct = CrossEntropyLoss()

from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=learning_rate)

from transformers import get_scheduler

num_training_steps = num_epochs * len(train_data)
num_validation_steps = num_epochs * len(validation_data)

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps
)

the_encoder = model.get_encoder()
the_decoder = model.get_decoder()
last_linear_layer = model.lm_head

model.train()

for name, param in model.named_parameters():
  param.requires_grad = True

torch.set_grad_enabled(True)

train_log = []

from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps + num_validation_steps))
import gc
for epoch in range(num_epochs):

    model.train()
    training_loss = 0.0
    validation_loss = 0.0
    print("Training")
    for batch in train_data:
      if torch.cuda.is_available():
        batch = {k: v.to('cuda') for k, v in batch.items()}

      encoder_output = the_encoder(input_ids = batch['input_ids'],
                                   attention_mask = batch['attention_mask'])
      
      decoder_output = the_decoder(input_ids=batch['decoder_input_ids'],
                                   attention_mask=batch['decoder_attention_mask'],
                                   encoder_hidden_states=encoder_output[0],
                                   encoder_attention_mask=batch['attention_mask'])

      decoder_output = decoder_output.last_hidden_state
      lm_head_output = last_linear_layer(decoder_output)

      loss = loss_fct(lm_head_output.view(-1, model.config.vocab_size),
                      batch['labels'].view(-1))
      # loss.requires_grad = True
      training_loss += loss.item()

      loss.backward()
      optimizer.step()
      lr_scheduler.step()
      optimizer.zero_grad()
      progress_bar.update(1)
    
    model.eval()
    print("Validating")
    for batch in validation_data:
        if torch.cuda.is_available():
          batch = {k: v.to('cuda') for k, v in batch.items()}
        
        with torch.no_grad():
            outputs = model(**batch)

        loss = outputs.loss
        validation_loss += loss
        progress_bar.update(1)
    
    training_loss = training_loss / len( train_data )
    validation_loss = validation_loss / len( validation_data )
    print("Epoch {}:\tTraining Loss {:.2f}\t/\tValidation Loss {:.2f}".format(epoch+1, training_loss, validation_loss))
    train_log.append("Epoch {}:\tTraining Loss {:.2f}\t/\tValidation Loss {:.2f}".format(epoch+1, training_loss, validation_loss))

torch.save(model.state_dict(),"indic_model.pth" )
import pandas as pd
import pickle
train_df = pd.read_csv(train_path)
valid_df = pd.read_csv(val_path)

preds, acts = [],[]
x,y = train_df["Article"], train_df["Summary"]
for i,j in zip(x[:1000],y[:1000]):
  inp = tokenizer(i, add_special_tokens=False, max_length=1024, return_tensors="pt").input_ids
  inp = inp.cuda()
  predicted = model.generate(inp, use_cache=True, num_beams=number_beams_eval, max_length=128, min_length=1, early_stopping=True)
  pred = tokenizer.decode(predicted[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)
  preds.append(pred)
  acts.append(j)
with open(f'{language}_train_predictions.pkl','wb') as f:
  pickle.dump(preds,f)
with open(f'{language}_train_actuals','wb') as f:
  pickle.dump(acts,f)

preds, acts = [],[]
x,y = valid_df["Article"], valid_df["Summary"]
for i,j in zip(x[:1000],y[:1000]):
  inp = tokenizer(i, add_special_tokens=False, max_length=1024, return_tensors="pt").input_ids
  inp = inp.cuda()
  predicted = model.generate(inp, use_cache=True, num_beams=number_beams_eval, max_length=128, min_length=1, early_stopping=True)
  pred = tokenizer.decode(predicted[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)
  preds.append(pred)
  acts.append(j)
with open(f'{language}_valid_predictions.pkl','wb') as f:
  pickle.dump(preds,f)
with open(f'{language}_valid_actuals','wb') as f:
  pickle.dump(acts,f)

with open(f'{language}_log.txt','w') as f:
  for line in train_log:
    f.write(f"{line}\n")
